# Crawler
> This is used for data collection, It will spontaneously crawl target website and store website data in database. Crawling comforms to robots.txt.
